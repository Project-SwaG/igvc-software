\section{Software Design}

\subsection{Architecture}

The software on the robot is split between algorithmic and control code that runs on the primary laptop and data acquisition code that runs on the microcontrollers. This allows the laptop to perform all of the intensive calculation allowing the use of very cheap microcontrollers that only need to perform basic low level actions.

The primary language in our system is C++. Object oriented approaches are used for programs that run on the laptop, while simplified imperative code is used on the microcontrollers to minimize overhead. Several standard system and computer graphic libraries are used in our code base. The Boost C++ library is used extensively to provide data structures, serial IO handlers and threads. Image processing is mostly done with algorithms built from elements provided by OpenCV, with some transforms offloaded to the GPU with OpenGL. The codebase currently only runs on Linux, however we could theoretically port the code base to run on other major platforms (Windows, MacOS, Solaris) with little effort.

\subsection{Algorithms}

\subsubsection{Vision}

The robot uses vision as the primary method of detecting obstacles and lines. The vision algorithm has been developed and modified over several years of competition and has proven to be reasonably robust. After passing the input video through several different algorithms, a short-term map of the world is created, which the robot is driven off of.

\begin{figure}[H]
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=2.5in]{./pics/raw.png}
\caption{Raw Camera Frame}
\label{FIG:Raw}
\end{minipage}
\hspace{0.1in}
\begin{minipage}[b]{0.5\linewidth}
\centering
\includegraphics[width=2.5in]{./pics/trans.png}
\caption{Transformed Camera Frame}
\label{FIG:Trans}
\end{minipage}
\end{figure}

The input video, Figure \ref{FIG:Raw}, is first passed through an inverse perspective transform, as seen in Figure \ref{FIG:Trans}. This transform makes both near and far off objects a normalized size, and makes the image appear to be taken from directly overhead. This flattened image assumes the course is a plane, which does cause distortion of the barrels, but this is accounted for in the mapping algorithm. The transformed image is much easier to process into a map than a normal, perspective image would be.

The image is then color segmented and thresholded based on the color that is centered directly in front of the robot, as seen in Figure \ref{FIG:ColorSeg}. Safe colors are marked white, the rest are black. The color is averaged in time between frames to allow for some variation in color, for example, if there is dead patch in the grass. This allows the robot to operate on many different surfaces with the same software. For testing we have operated on asphalt parking lots, navigating between the lines marking parking spaces.

\begin{figure}[H]
\begin{center}
\includegraphics[width=4in]{./pics/thresh.png}
\caption{Color Segmentation Output}
\label{FIG:ColorSeg}
\end{center}
\end{figure}

After converting the transformed image to grayscale, feature tracking is performed between subsequent frames. The tracked features are denoted by the black lines in the above grayscale image. The algorithm looks for features that have been translated and rotated between frames. This allows us to build a set of likely planar homographies between the images, which can be backed out into likely robot motion between frames. The possible planar homographies often include several incorrectly matched points, so Random Sample Consensus (RANSAC), an algorithm good at outlier handling, is used to reject the outliers and select the best transform.

\begin{figure}[H]
\begin{center}
\includegraphics[width=4in]{./pics/feature.png}
\caption{Feature Tracker Output}
\label{FIG:Feature}
\end{center}
\end{figure}

Using motion data, camera frames are drawn into the world map, shown in Figure \ref{FIG:Map}. The map is a grayscale image, and represents a probability function of traversablility, where black (0) represents non-traversable, gray (127) represents unknown areas and white(255) represents traversable areas. The map is built up as the robot moves, and slowly decays back to gray to prevent loop closure errors from building up. This map allows the robot to avoid obstacles which are no longer in the current camera frame. The robot is driven from the map, by a path planning algorithm.

\begin{figure}[H]
\begin{center}
\includegraphics[width=4in]{./pics/map.png}
\caption{World Map}
\label{FIG:Map}
\end{center}
\end{figure}

\subsubsection{Path Planning}

Once the world map is generated, a potential fields algorithm is used. The robot is attracted to the nearest GPS waypoint. Obstacles repel the robot, and the strength of the the field is higher if the object is closer. For speed, a vectorized implementation of potential fields was used. We also support A* for planning using the vectorized potential fields as a cost function. 

\subsubsection{Positioning}

The data from the GPS, optical encoders, and magnetometer is used to estimate the robot's current position in real world coordinates. It uses a special non-linear filter to combine the inputs into a single estimator, accounting for the differences in accuracy between different sensors. We plan to extend this to a full Kalman filter in future competitions.

\subsubsection{LIDAR}

The LIDAR is used as an obstacle and ramp detection sensor. The incoming range information is filtered with a running average to reduce random noise, and is then passed through an erosion-dilation filter to remove isolated points. The $2^{nd}$ derivative is then calculated and thresholded to look for linear objects similar in width to the ramp which when found are given to the path planner as a special goal. Returns with non-zero $2^{nd}$ derivatives are interpreted and given to the path planner as objects to be avoided.
